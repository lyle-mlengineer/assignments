{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5288a080",
   "metadata": {},
   "source": [
    "# DATA WAREHOUSING\n",
    "\n",
    "**What is a Data Warehouse**\n",
    "- A data warehouse is a centralized system that collects and stores large amounts of historical data from various sources, such as sales, marketing, and finance systems, to support business intelligence, reporting, and analytics. \n",
    "- It serves as a single source of truth for an organization, providing a unified and consistent view of data over time to help business users make informed decisions. \n",
    "- Data warehouses differ from operational databases, which support daily operations, by being optimized for analytical queries and long-range historical analysis.  \n",
    "\n",
    "**Key characteristics:**\n",
    "\n",
    "- **Centralized Repository**: It consolidates data from multiple sources into one location. \n",
    "- **Integrated Data**: Data is cleaned, transformed, and standardized into a consistent format for easier analysis. \n",
    "- **Historical Data**: It stores vast amounts of past data, allowing for trend analysis and long-term insights. \n",
    "- **Non-Volatile**: Data in a data warehouse is typically not updated or deleted once stored, preserving historical context. \n",
    "- **Subject-Oriented**: It focuses on specific business subjects, like sales or customer behavior, rather than daily transactions. \n",
    "\n",
    "**Purpose and Use:**\n",
    "\n",
    "- **Business Intelligence (BI)**: Data warehouses are a foundational component of BI, providing the data for dashboards, reports, and analytics tools. \n",
    "- **Informed Decision-Making**: By providing a comprehensive, integrated view of historical and current data, they help businesses understand performance, identify trends, and make smarter decisions. \n",
    "- **Data Mining and Analytics**: It provides a rich dataset for data scientists and analysts to perform data mining, data visualization, and advanced analytics. \n",
    "\n",
    "**How it works (ETL):**\n",
    "\n",
    "- **Extract**: Data is extracted from various source systems (e.g., databases, applications, CRM systems). \n",
    "- **Transform**: The extracted data is cleaned, standardized, and formatted into a common structure. \n",
    "- **Load**: The transformed data is then loaded into the data warehouse for analysis. \n",
    "\n",
    "In essence, a data warehouse acts as a strategic data hub, transforming raw operational data into actionable insights that drive business growth and competitive advantage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d8139",
   "metadata": {},
   "source": [
    "## ETL Techniques\n",
    "- ETL (Extract, Transform, Load) is a data integration technique that involves Extracting data from various sources, Transforming it into a usable format by cleansing and structuring it, and then Loading it into a target system, like a data warehouse. \n",
    "- Key ETL techniques include ``batch processing``, for periodic data transfers; ``incremental ETL`` using change data capture to only process new or changed data; ``data cleansing`` to ensure accuracy; and ``data profiling`` to understand data quality before processing.  \n",
    "\n",
    "**Key ETL Techniques**\n",
    "\n",
    "1. **Extraction**: \n",
    "This involves pulling raw data from different sources, such as CRMs, ERPs, or flat files. \n",
    "- ``Full Extraction``: Retrieves all the data from the source system at once. \n",
    "- ``Incremental Extraction (Change Data Capture)``: Uses time or date-based tools to identify and extract only the data that has changed since the last extraction. \n",
    "- ``API Extraction``: Utilizes Application Programming Interfaces to communicate with software and operating systems to extract data. \n",
    "2. **Transformation**: \n",
    "Here, the extracted data is manipulated, cleaned, and standardized. \n",
    "- ``Data Cleansing``: Corrects errors, removes duplicates, and standardizes data formats to improve quality. \n",
    "- ``Data Derivation``: Creates new data fields by applying business rules or calculations to existing data. \n",
    "- ``Data Aggregation``: Summarizes data into a more concise form by grouping and combining related records. \n",
    "- ``Data Integration``: Merges data from different sources into a single, unified dataset. \n",
    "- ``Data Filtering``: Selects only the relevant data based on specific criteria for the target system. \n",
    "3. **Loading**: \n",
    "This is the process of storing the transformed data into a target destination, typically a data warehouse or data lake. \n",
    "- ``Full Load``: Writes the entire dataset into the target system. \n",
    "- ``Incremental Load``: Updates or adds new data to the existing target system based on changes in the source data. \n",
    "\n",
    "**Common Approaches & Considerations**\n",
    "\n",
    "1. ``Staging Area``: A temporary, intermediate storage location for extracted data before it is transformed and loaded. \n",
    "2. ``Batch ET``L: Processes large volumes of data in batches at scheduled intervals. \n",
    "3. ``ETL Tools``: Software like Informatica, Talend, and Microsoft SSIS provides user-friendly interfaces and connectors to facilitate the ETL process. \n",
    "4. ``ETL vs. ELT``: A contrasting approach where data is first loaded into the target system and then transformed within the data storage solution, especially common with cloud data lakes and unstructured data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a150c3a",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "This project uses:\n",
    "- ``PostgreSQL``: Lightweight server for hosting your SQL database.\n",
    "- ``PGAdmin``: GUI for managing and interacting with databases.\n",
    "\n",
    "To setup up your system:\n",
    "1. Download and install PostgreSQL from this [link](https://www.enterprisedb.com/downloads/postgres-postgresql-downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925913a8",
   "metadata": {},
   "source": [
    "## Building the Data Warehouse\n",
    "The aim is to develop a modern data warehouse using SQL Server to consolidate sales data, enabling analytical reporting.\n",
    "\n",
    "- **Data Sources**: Import data from two source systems (ERP and CRM) provided as CSV files.\n",
    "- **Data Quality**: Cleanse and resolve data quality issues before analysis.\n",
    "- **Integration**: Combine both sources into a user-friendly data model for analytical queries.\n",
    "- **Scope**: Focus on the latest dataset only; historization of data is not required.\n",
    "- **Documentation**: Provide clear data model documentation to support both business stakeholders and analytics teams.\n",
    "\n",
    "There are four different approaches to building a data warehouse:\n",
    "1. InMon\n",
    "2. Kimball\n",
    "3. Data Vault\n",
    "4. Medallion\n",
    "\n",
    "**The Medallion architerture**\n",
    "A medallion architecture serves as a data design blueprint tailored for organizing data within a lake house environment. Its primary aim is to enhance the structure and quality of data gradually as it traverses through successive layers of the architecture, progressing from Bronze to Silver to Gold layers.\n",
    "\n",
    "1. **Bronze layer**\n",
    "The Bronze layer serves as the initial landing ground for all data originating from external source systems. Datasets within this layer mirror the structures of the source system tables in their original state, supplemented by extra metadata columns such as load date/time and process ID. The primary emphasis here is on Change Data Capture, enabling historical archiving of the source data, maintaining data lineage, facilitating audit trails, and allowing for reprocessing if necessary without requiring a fresh read from the source system.\n",
    "\n",
    "2. **Silver layer**\n",
    "The next layer of the warehouse is the Silver layer. Within this layer, data from the Bronze layer undergoes a series of operations to a “just-enough” state (which will be discussed in detail later). This prepares the data in the Silver layer to offer an encompassing “enterprise view” comprising essential business entities, concepts, and transactions.\n",
    "\n",
    "3. **Gold layer**\n",
    "The last layer of the warehouse is the Gold layer. Data within the Gold layer is typically structured into subject area specific databases, primed for consumption. This layer is dedicated to reporting and employs denormalized, read-optimized data models with minimal joins. It serves as the ultimate stage for applying data transformations and quality rules. Commonly, you will observe the integration of Kimball-style star schema based data marts within the Gold Layer of the warehouse.\n",
    "\n",
    "![Alt text](images/medallion.png \"Optional Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b5fa2",
   "metadata": {},
   "source": [
    "## Data Warehouse Structure\n",
    "\n",
    "![Alt text](images/warehouse.png \"Optional Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485915f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
