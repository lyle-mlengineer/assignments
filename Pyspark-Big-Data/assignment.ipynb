{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9558c4",
   "metadata": {},
   "source": [
    "# Programming for Massive Data\n",
    "\n",
    "This notebook is to be run using google colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01860025",
   "metadata": {},
   "source": [
    "## Introduction to Massive Data Programming\n",
    "\n",
    "- Massive data, more commonly known as ``big data`` refers to extremely large and complex datasets that cannot be stored, processed, or analyzed using traditional data management tools.\n",
    "\n",
    "### Characteristics of massive data (The 3Vs)\n",
    "The three defining characteristics explain why massive data is fundamentally different from traditional data and requires a new approach to management and analysis.\n",
    "\n",
    "**1. Volume**\n",
    "\n",
    "- Volume refers to the enormous amount of data generated every second from a wide array of sources. It is measured in petabytes and even zettabytes, a scale that traditional data storage systems cannot manage. \n",
    "- Example: A social media platform like Facebook ingests over 500 terabytes of new data every day through photo uploads, message exchanges, and comments. \n",
    "\n",
    "**2. Velocity**\n",
    "\n",
    "- Velocity is the rapid rate at which data is created, collected, and needs to be processed. In many applications, this requires near-real-time or real-time processing to have any meaningful impact. \n",
    "- Example: High-frequency stock trading generates real-time market data that must be analyzed and acted upon instantly. \n",
    "\n",
    "**3. Variety**\n",
    "\n",
    "- Variety highlights the many different types and formats of data, which can be structured, semi-structured, or unstructured.Traditional systems were built to handle only structured data that fits neatly into tables. \n",
    "1. ``Structured data``: Highly organized and easy to search, such as data in a relational database or a spreadsheet.\n",
    "2. ``Semi-structured data``: A hybrid data type that has some organizational properties but no rigid schema. An email, for example, has structured elements (sender, recipient, timestamp) and unstructured text in the message body.\n",
    "3. ``Unstructured data``: Data with no predefined format or organization, making it the most challenging to process. Examples include text documents, images, audio, and video files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8b9a6",
   "metadata": {},
   "source": [
    "### Challenges of traditional programming for large datasets.\n",
    "\n",
    "1. Data Volume and Storage:\n",
    "\n",
    "- Inadequate Storage Solutions: Traditional databases and file systems are not designed to efficiently store and manage petabytes or exabytes of data.\n",
    "- Cost and Complexity: Scaling traditional storage infrastructure to accommodate massive data volumes becomes prohibitively expensive and complex to maintain.\n",
    "\n",
    "2. Data Velocity and Processing:\n",
    "\n",
    "- Real-time Processing Limitations:\n",
    "Traditional systems struggle to process data streams generated at high velocity in real-time, hindering immediate insights and reactions.\n",
    "- Performance Bottlenecks:\n",
    "Conventional processing methods become slow and inefficient when faced with the sheer volume and speed of Big Data, leading to delays in analysis.\n",
    "3. Data Variety and Management:\n",
    "- Handling Unstructured Data:\n",
    "Traditional programming is primarily suited for structured data, struggling to manage and analyze diverse formats like text, images, video, and sensor data.\n",
    "- Data Integration Challenges:\n",
    "Integrating data from multiple, heterogeneous sources into a unified view for analysis is complex and time-consuming in traditional setups.\n",
    "4. Data Quality and Veracity:\n",
    "- Ensuring Data Accuracy:\n",
    "Maintaining data quality, consistency, and accuracy across vast and diverse datasets is a significant challenge, as errors can lead to flawed analyses.\n",
    "- Manual Data Preparation:\n",
    "Traditional methods often require extensive manual effort for data cleaning, transformation, and enrichment, which is impractical for large datasets.\n",
    "5. Scalability and Flexibility:\n",
    "- Limited Scalability:\n",
    "Traditional applications are often not designed to scale horizontally to handle ever-increasing data volumes and processing demands.\n",
    "- Inflexibility to Changes:\n",
    "Adapting traditional programs to new data types, sources, or analysis requirements can be cumbersome and require extensive reprogramming.\n",
    "6. Computational Resources and Cost:\n",
    "- Resource Intensive:\n",
    "Processing large datasets with traditional methods can demand significant computational power, potentially leading to high infrastructure costs.\n",
    "- Lack of Optimization:\n",
    "Traditional programming may not fully leverage techniques like parallel processing or distributed computing, leading to inefficient resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607fd67",
   "metadata": {},
   "source": [
    "### Introduction to distributed computing concepts.\n",
    "\n",
    "- Distributed computing involves multiple, independent computers connected by a network that work together to solve a problem as a single, coherent system. \n",
    "- This approach provides benefits like enhanced scalability by adding more devices, fault tolerance so the system continues to run even if one computer fails, and increased performance through parallel processing.\n",
    "\n",
    "#### Core Concepts\n",
    "- ``Nodes``: Individual computers or devices that are part of the distributed system. \n",
    "Message Passing: Nodes communicate and coordinate their actions by sending messages to each other over the network. \n",
    "- ``Shared State``: The components of the system maintain a shared understanding of the overall state, even though they are physically separated. \n",
    "- ``Independence``: Each node can operate independently, processing its own data. \n",
    "Unified Appearance: To the end-user, the entire distributed system appears as a single, cohesive computer or resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4182a7f",
   "metadata": {},
   "source": [
    "## Programming for Distributed Systems\n",
    "- The Hadoop ecosystem provides a framework for big data. \n",
    "- HDFS ``(Hadoop Distributed File System)`` is the storage layer, breaking large files into blocks and distributing them across a cluster with replication for fault tolerance. \n",
    "- ``MapReduce`` is the processing layer, a parallel programming model that transforms data through a ``map`` phase and aggregates results in a ``reduce`` phase, allowing complex computations on distributed data.  \n",
    "\n",
    "#### HDFS (Hadoop Distributed File System)\n",
    "- Purpose:\n",
    "HDFS is a distributed file system designed to store very large datasets across a cluster of commodity hardware. \n",
    "- Key Features:\n",
    "1. Large Blocks: HDFS breaks large files into large blocks (e.g., 128MB or more), reducing overhead compared to traditional file systems. \n",
    "2. Replication: To ensure fault tolerance, HDFS replicates each data block on multiple nodes in the cluster. \n",
    "3. Scalability: HDFS can scale to thousands of nodes and petabytes of data. \n",
    "4. Data Locality: It promotes data locality by moving computation to where the data resides, minimizing network congestion. \n",
    "\n",
    "#### MapReduce\n",
    "- Purpose:\n",
    "MapReduce is a programming model and processing framework for performing large-scale, parallel data analysis. \n",
    "- How it Works:\n",
    "1. Map Phase: Input data is processed in parallel by \"mapper\" functions, which transform the data into key-value pairs. \n",
    "Shuffle and Sort: A shuffle and sort process organizes these key-value pairs. \n",
    "2. Reduce Phase: \"Reducer\" functions then aggregate and process the sorted pairs to produce the final output, which is stored back in HDFS. \n",
    "- Benefits:\n",
    "1. MapReduce abstracts the complexity of parallel processing, allowing developers to focus on the business logic rather than the intricacies of distributed computation. \n",
    "\n",
    "#### The Hadoop Ecosystem \n",
    "- The Hadoop ecosystem is a collection of projects and tools that complement the core components (HDFS, MapReduce, and YARN) to provide a complete solution for big data problems. \n",
    "- Other components include Spark (for in-memory processing), Hive and Pig (for query-based data processing), and HBase (a NoSQL database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a7933",
   "metadata": {},
   "source": [
    "## Apache Spark - Introduction\n",
    "\n",
    "- Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters i.e it's used to process very large datasets that cannot fit into a single computers' memory. For example 10GB of transaction files for an eccomerce company\n",
    "- Its core components and libraries include:\n",
    "    - Structured Streaming\n",
    "    - Advanced Analytics\n",
    "    - Libraries and Ecosystems\n",
    "    - Datasets, DataFrames and SQL\n",
    "    - RDDs and Distributed Variables\n",
    "- It can be used by various prograaming languages including Python, Scala and Java over various interfaces for example Intergradet Development Environment like VSCode and using notebooks such as Google Colab\n",
    "- Apache spark can load and store various data formats such as csv and parquet. It is also used in analytics, both batch and streaming, supports SQL and can be used to build and evaluate Machine learning models.\n",
    "- It supports various backend storage services such as Amazon s3, works with message buses such as Apache kafka, real time analytics frameworks such as Apache Flink as well as databases such as Apache Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24529de4",
   "metadata": {},
   "source": [
    "## Apache Spark - Setting Up\n",
    "To run Apache spark on Google Colab, simply run the cell below to install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531075b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pyspark notebook pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on Colab\n",
    "# Make sure to upload the data folder to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on local computer\n",
    "# BASE_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1f34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302aa5a",
   "metadata": {},
   "source": [
    "## Testing out installed pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd1fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/16 18:23:39 WARN Utils: Your hostname, lyle-ThinkPad-T470s-W10DG, resolves to a loopback address: 127.0.1.1; using 172.22.0.1 instead (on interface br-13a8da3b93e6)\n",
      "25/09/16 18:23:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/16 18:23:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark and initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c420217",
   "metadata": {},
   "source": [
    "## Spark’s Basic Architecture\n",
    "- Apache spark uses a group of computers, called a cluster in order to manage the processing of vary large datasets.\n",
    "- Spark applications consist of:\n",
    "    1. A ``Driver Process``\n",
    "    2. A set of ``Executor Processes``\n",
    "- The ``Driver Process`` runs your ``main() function``, sits on a node in the cluster, and is responsible for three things:\n",
    "    1. Maintaining information about the Spark Application\n",
    "    2. Responding to a user’s program or input\n",
    "    3. Analyzing, distributing, and scheduling work across the executors\n",
    "- The ``executors`` are responsible for actually carrying out the work that the driver assigns them. This means that each executor is responsible for only two things: executing code assigned to it by the driver, and reporting the state of the computation on that executor back to the driver node.\n",
    "## The SparkSession\n",
    "You control your Spark Application through a driver process called the SparkSession. The SparkSession instance is the way Spark executes user-defined manipulations across the cluster. Here's hpw to get a spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45c4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark and initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106aebea",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "- In Apache Spark, a Resilient Distributed Dataset (RDD) is a fundamental, fault-tolerant collection of data that can be processed in parallel across a cluster. \n",
    "- RDD operations are divided into two categories: \n",
    "1. Transformations\n",
    "2. Actions. \n",
    "\n",
    "#### RDD characteristics\n",
    "\n",
    "- Resilient: RDDs can recover from node failures. Spark tracks the lineage of all transformations, allowing it to re-create lost partitions of data if a node fails.\n",
    "- Distributed: The data in an RDD is logically partitioned and distributed across multiple nodes in a cluster, enabling parallel processing.\n",
    "- Immutable: Once created, an RDD cannot be changed. Any operation on an RDD results in a new RDD.\n",
    "- Lazy Evaluation: Transformations on an RDD are not executed immediately. Instead, Spark records the transformations as a Directed Acyclic Graph (DAG) and only computes the data when an action is called.\n",
    "- In-Memory Computing: RDDs can be cached in memory for much faster access in iterative algorithms and interactive data mining. \n",
    "\n",
    "#### RDD operations\n",
    "- RDD operations are classified as ``transformations`` or ``actions``. \n",
    "\n",
    "#### Creating RDDS\n",
    "1. From a local collection\n",
    "2. From an external dataset (text file)\n",
    "3. From dataframes and datasets\n",
    "4. From other RDDS (transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9f59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateRDDExample\").getOrCreate()\n",
    "# Create an RDD from a local collection\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "# Show the elements of the RDD\n",
    "for element in rdd.collect():\n",
    " print(element)\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138f4177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **TimeSnap**: A playful name that suggests capturing moments in time.\n",
      "2. **TubeTimer**: A straightforward name that combines \"tube\" (YouTube) with \"timer\".\n",
      "3. **Stampify**: A fun, catchy name that implies converting video content into timestamped moments.\n",
      "4. **TimeTag**: A simple, memorable name that conveys the idea of tagging moments with timestamps.\n",
      "5. **VidMarks**: A name that suggests marking or bookmarking important moments in videos.\n",
      "6. **Timeline**: A name that emphasizes the API's focus on extracting timestamps and creating a timeline of events.\n",
      "7. **ClipClock**: A name that combines \"clip\" (a short video segment) with \"clock\" to convey the idea of timing.\n",
      "8. **Momenta**: A name that suggests capturing and extracting meaningful moments from videos.\n",
      "9. **TimeSeek**: A name that implies searching for and finding specific moments in time.\n",
      "10. **StampKit**: A name that suggests a toolkit for extracting and working with timestamps.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateRDDFromFileExample\").getOrCreate()\n",
    "# Create an RDD from a text file\n",
    "file_name = os.path.join(BASE_DIR, \"timestamps.txt\")\n",
    "rdd = spark.sparkContext.textFile(file_name)\n",
    "# Show the first few elements of the RDD\n",
    "for line in rdd.take(10):\n",
    " print(line)\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359c1e2",
   "metadata": {},
   "source": [
    "#### Partitions\n",
    "- To allow every executor to perform work in parallel, Spark breaks up the data into chunks called partitions. \n",
    "- A partition is a collection of rows that sit on one physical machine in your cluster. \n",
    "- A DataFrame’s partitions represent how the data is physically distributed across the cluster of machines during execution.\n",
    "\n",
    "#### Transformations\n",
    "Are instructions to pyspark on how to change a given dataframe. Transformations do not change the undrlying dataframe until an ``action`` is called on them. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f64dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/15 11:58:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "\n",
      "DataFrame[number: bigint]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "numbers = spark.range(10).toDF(\"number\")\n",
    "\n",
    "# Show the DataFrame\n",
    "numbers.show()\n",
    "\n",
    "# Filter numbers divisible by 3\n",
    "divisible_by_3 = numbers.filter(numbers.number % 3 == 0)\n",
    "# Will not execute until an action is called\n",
    "print(divisible_by_3)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c24df0",
   "metadata": {},
   "source": [
    "#### Examples of Transformations:\n",
    "- ``map()``: Applies a function to each element of an RDD/DataFrame. \n",
    "- ``filter():`` Returns a new RDD/DataFrame containing only elements that satisfy a given condition. \n",
    "- ``select()``: Selects specified columns from a DataFrame.\n",
    "- ``withColumn()``: Adds a new column or replaces an existing one in a DataFrame.\n",
    "- ``groupBy()``: Groups data based on specified columns for aggregation.\n",
    "- ``join()``: Combines two DataFrames based on a join expression.\n",
    "- ``union()``: Combines two DataFrames with the same schema.\n",
    "- ``distinct()``: Returns a new RDD/DataFrame with unique elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c4cf4",
   "metadata": {},
   "source": [
    "#### Lazy Evaluation\n",
    "Lazy evaulation means that Spark will wait until the very last moment to execute the graph of computation instructions. In Spark, instead of modifying the data immediately when you express some operation, you build up a plan of transformations that you would like to apply to your source data. By waiting until the last minute to execute the code, Spark compiles this plan from your raw DataFrame transformations to a streamlined physical plan that will run as efficiently as possible across the cluster.\n",
    "\n",
    "#### Actions\n",
    "Transformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. An action instructs Spark to compute a result from a series of transformations.The simplest action is ``count``, which gives us the total number of records in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d32cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "\n",
      "Count of numbers divisible by 3: 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "numbers = spark.range(10).toDF(\"number\")\n",
    "\n",
    "# Show the DataFrame\n",
    "numbers.show()\n",
    "\n",
    "# Filter numbers divisible by 3\n",
    "divisible_by_3 = numbers.filter(numbers.number % 3 == 0)\n",
    "# Use the count action to trigger execution\n",
    "count_divisible_by_3 = divisible_by_3.count()\n",
    "print(f\"Count of numbers divisible by 3: {count_divisible_by_3}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75566b",
   "metadata": {},
   "source": [
    "The output of the preceding code should be ``4``. Of course, count is not the only action. There are three kinds of actions:\n",
    "- Actions to view data in the console\n",
    "- Actions to collect data to native objects in the respective language\n",
    "- Actions to write to output data sources\n",
    "\n",
    "In specifying this action, we started a Spark job that runs our filter transformation (a narrow transformation), then an aggregation (a wide transformation) that performs the counts on a per partition basis, and then a collect, which brings our result to a native object in the respective language.\n",
    "\n",
    "#### Examples of Actions:\n",
    "- ``collect()``: Returns all elements of an RDD/DataFrame as a list to the driver program. Use with caution on large datasets.\n",
    "- ``show()``: Displays the top rows of a DataFrame.\n",
    "- ``count()``: Returns the number of elements in an RDD/DataFrame.\n",
    "- ``take(n)``: Returns the first n elements of an RDD/DataFrame.\n",
    "- ``first() / head()``: Returns the first row or the first n rows of a DataFrame.\n",
    "- ``reduce()``: Aggregates elements of an RDD using a specified function.\n",
    "- ``saveAsTextFile() / write.format().save()``: Writes the RDD/DataFrame to an external storage system (e.g., HDFS, S3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bfc4d",
   "metadata": {},
   "source": [
    "## Working with DataFrames and Spark SQL\n",
    "\n",
    "- Apache Spark DataFrames are a distributed collection of data organized into named columns, conceptually similar to a table in a relational database or a data frame in R/Python (e.g., Pandas). \n",
    "- They provide a higher-level abstraction than Resilient Distributed Datasets (RDDs) and are designed to simplify working with structured and semi-structured data in Spark. DataFrames are available in Scala, Java, Python, and R.\n",
    "#### Advantages of DataFrames over RDDs\n",
    "DataFrames offer several key advantages over RDDs:\n",
    "1. Optimized Execution with Catalyst Optimizer:\n",
    "DataFrames leverage Spark's Catalyst Optimizer, which analyzes queries and generates optimized execution plans. This includes techniques like predicate pushdown, column pruning, and join reordering, leading to significantly faster and more efficient processing compared to manual RDD transformations.\n",
    "2. Schema Awareness and Type Safety (with Datasets):\n",
    "While RDDs are untyped collections of objects, DataFrames introduce a schema, providing information about column names and data types. This enables Spark to perform optimizations and error checking. When using Datasets (an extension of DataFrames in Scala and Java), compile-time type safety is also provided, catching errors earlier in the development cycle.\n",
    "SQL-like Interface:\n",
    "3. DataFrames provide a user-friendly, SQL-like interface for data manipulation. This allows users to express complex transformations and queries using familiar SQL syntax or a DataFrame API that mirrors common database operations, making it more accessible to a broader range of users, including data analysts.\n",
    "4. Integration with Spark Ecosystem:\n",
    "DataFrames seamlessly integrate with other Spark libraries like Spark SQL, MLlib (Machine Learning Library), and GraphX, providing a unified and consistent API for various data processing tasks.\n",
    "5. Easier Data Source Integration:\n",
    "DataFrames simplify reading from and writing to various data sources (e.g., CSV, JSON, Parquet, Avro, ORC) through a unified API, making data ingestion and output more streamlined.\n",
    "\n",
    "#### How to Create Dataframes\n",
    "1. From a list of iterables\n",
    "2. From a list of tuples\n",
    "3. From a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbe64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+----------+\n",
      "| id|   name|age|       dob|\n",
      "+---+-------+---+----------+\n",
      "|  1|  Alice| 30|1994-05-15|\n",
      "|  2|    Bob| 24|2000-01-20|\n",
      "|  3|Charlie| 35|1989-11-01|\n",
      "+---+-------+---+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### From a list of rows\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameCreation\").getOrCreate()\n",
    "\n",
    "# Create data as a list of Row objects\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", age=30, dob=date(1994, 5, 15)),\n",
    "    Row(id=2, name=\"Bob\", age=24, dob=date(2000, 1, 20)),\n",
    "    Row(id=3, name=\"Charlie\", age=35, dob=date(1989, 11, 1))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0350f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 24|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 24|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From a list of tuples\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameCreation\").getOrCreate()\n",
    "\n",
    "# Data as a list of tuples\n",
    "data_tuples = [\n",
    "    (1, \"Alice\", 30),\n",
    "    (2, \"Bob\", 24),\n",
    "    (3, \"Charlie\", 35)\n",
    "]\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "df_schema = spark.createDataFrame(data_tuples, schema=schema)\n",
    "\n",
    "# Create DataFrame with inferred schema (if schema is omitted)\n",
    "df_inferred = spark.createDataFrame(data_tuples, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "df_schema.show()\n",
    "df_schema.printSchema()\n",
    "\n",
    "df_inferred.show()\n",
    "df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e14a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   A|\n",
      "|   2|   B|\n",
      "|   3|   C|\n",
      "+----+----+\n",
      "\n",
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From a pandas DataFrame\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameCreation\").getOrCreate()\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'col1': [1, 2, 3],\n",
    "    'col2': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "spark_df.show()\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551f4ff",
   "metadata": {},
   "source": [
    "## DataFrames and SQL\n",
    "We can also use SQL with dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26d23c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|DEST_COUNTRY_NAME|flight_count|\n",
      "+-----------------+------------+\n",
      "|         Anguilla|           1|\n",
      "|           Russia|           1|\n",
      "|         Paraguay|           1|\n",
      "|          Senegal|           1|\n",
      "|           Sweden|           1|\n",
      "+-----------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "# Read a CSV file into a DataFrame\n",
    "file_name = os.path.join(BASE_DIR, \"flight-data\", \"csv\", \"2015-summary.csv\")\n",
    "df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "\n",
    "# Create a temporary view for SQL queries\n",
    "df.createOrReplaceTempView(\"flights\")\n",
    "# Execute an SQL query\n",
    "sqlDF = spark.sql(\"SELECT DEST_COUNTRY_NAME, count(1) AS flight_count FROM flights GROUP BY DEST_COUNTRY_NAME\")\n",
    "# Show the results\n",
    "sqlDF.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a951fd",
   "metadata": {},
   "source": [
    "#### Pyspark SQL functions\n",
    "\n",
    "1. Basic Functions:\n",
    "- ``col(column_name) / column(column_name)``: Returns a Column object based on the given name.\n",
    "- ``lit(value)``: Creates a Column of a literal value (e.g., string, integer, boolean).\n",
    "- ``alias(name):`` Renames a column.\n",
    "2. String Functions:\n",
    "- ``concat(*cols)``: Concatenates multiple columns into a single string column.\n",
    "- ``substring(str, pos, len)``: Extracts a substring from a string column.\n",
    "- ``lower(col) / upper(col)``: Converts a string column to lowercase or uppercase.\n",
    "- ``trim(col)``: Removes leading and trailing whitespace from a string column.\n",
    "- ``like(column, pattern)``: Filters rows where a string column matches a SQL LIKE pattern.\n",
    "3. Numeric/Math Functions:\n",
    "- ``abs(col)``: Computes the absolute value.\n",
    "- ``round(col, scale)``: Rounds a numeric column to a specified scale.\n",
    "- ``sqrt(col)``: Computes the square root.\n",
    "- ``ceil(col) / floor(col)``: Computes the ceiling or floor of a numeric column.\n",
    "4. Date and Time Functions:\n",
    "- ``current_date() / current_timestamp()``: Returns the current date or timestamp.\n",
    "- ``date_add(date, days) / date_sub(date, days)``: Adds or subtracts days from a date column.\n",
    "- ``year(col) / month(col) / dayofmonth(col)``: Extracts year, month, or day from a date column.\n",
    "5. Conditional Functions:\n",
    "- ``when(condition, value) / otherwise(value)``: Implements conditional logic similar to SQL's CASE WHEN.\n",
    "- ``isnull(col) / isnotnull(col)``: Checks for null or non-null values.\n",
    "6. Aggregate Functions:\n",
    "- ``count(col) / countDistinct(col)``: Counts non-null values or distinct values.\n",
    "- ``sum(col) / avg(col)``: Calculates the sum or average.\n",
    "- ``min(col) / max(col)``: Finds the minimum or maximum value.\n",
    "7. Array and Map Functions:\n",
    "- ``array(*cols)``: Creates an array column from multiple input columns.\n",
    "- ``explode(array_col)``: Expands an array column into separate rows for each element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5df793",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Here are a couple of exercises for working with spark and sql:\n",
    "1. Working with Booleans\n",
    "2. Working with Strings\n",
    "3. Working with dates and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5420135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "\n",
    "file_name = os.path.join(BASE_DIR, \"retail-data\", \"by-day\", \"2010-12-01.csv\")\n",
    "# Read a CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(file_name)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d353cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Working with booleans\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e19a08d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "|Set 7 Babushka Ne...|\n",
      "|Glass Star Froste...|\n",
      "|Hand Warmer Union...|\n",
      "|Hand Warmer Red P...|\n",
      "|Assorted Colour B...|\n",
      "|Poppy's Playhouse...|\n",
      "|Poppy's Playhouse...|\n",
      "|Feltcraft Princes...|\n",
      "|Ivory Knitted Mug...|\n",
      "|Box Of 6 Assorted...|\n",
      "|Box Of Vintage Ji...|\n",
      "|Box Of Vintage Al...|\n",
      "|Home Building Blo...|\n",
      "|Love Building Blo...|\n",
      "|Recipe Box With M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Working with strings\n",
    "# The initcap function will capitalize every word in a given string when that word is separated from another by a space\n",
    "\n",
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show()\n",
    "\n",
    "# in SQL 'SELECT initcap(Description) FROM dfTable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "105e3976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Upper and lowercase strings\n",
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.select(col(\"Description\"),\n",
    "lower(col(\"Description\")),\n",
    "upper(lower(col(\"Description\")))).show(2)\n",
    "\n",
    "# in SQL SELECT Description, lower(Description), Upper(lower(Description)) FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a22da1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Another trivial task is adding or removing spaces around a string. You can do this by using lpad, ltrim, rpad and rtrim, trim:\n",
    "\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80740deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working with dates\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d50c01d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2025-09-10|        2025-09-20|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# add and subtract five days from today\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a6bfc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# difference between two dates\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75014871",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "- Aggregating is the act of collecting something together and is a cornerstone of big data analytics.\n",
    "- In an aggregation, you will specify a key or grouping and an aggregation function that specifies\n",
    "how you should transform one or more columns. \n",
    "- This function must produce one result for each group, given multiple input values.\n",
    "-  Spark also allows us to create the following groupings types:\n",
    "The simplest grouping is to just summarize a complete DataFrame by performing an\n",
    "aggregation in a select statement.\n",
    "- A ``group by`` allows you to specify one or more keys as well as one or more\n",
    "aggregation functions to transform the value columns.\n",
    "- A ``window`` gives you the ability to specify one or more keys as well as one or more\n",
    "aggregation functions to transform the value columns. However, the rows input to the\n",
    "function are somehow related to the current row.\n",
    "- A ``grouping set`` which you can use to aggregate at multiple different levels. Grouping\n",
    "sets are available as a primitive in SQL and via rollups and cubes in DataFrames.\n",
    "- A ``rollup`` makes it possible for you to specify one or more keys as well as one or more\n",
    "aggregation functions to transform the value columns, which will be summarized\n",
    "hierarchically.\n",
    "- A ``cube`` allows you to specify one or more keys as well as one or more aggregation\n",
    "functions to transform the value columns, which will be summarized across all\n",
    "combinations of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dca8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/16 18:30:12 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/retail-data/all/*.csv.\n",
      "java.io.FileNotFoundException: File data/retail-data/all/*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in purchase data\n",
    "\n",
    "file_name = os.path.join(BASE_DIR, \"retail-data\", \"all\", \"*.csv\")\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(file_name)\\\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9aa4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show() # 541909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f79825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count distinct\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show() # 4070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07bff53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# First and last value\n",
    "\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f14cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Min and max\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bf0180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show() # 5176450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd79b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average\n",
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "df.select(\n",
    "count(\"Quantity\").alias(\"total_transactions\"),\n",
    "sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "\"total_purchases/total_transactions\",\n",
    "\"avg_purchases\",\n",
    "\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdb09b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# variance and standard deviation\n",
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e84cdb",
   "metadata": {},
   "source": [
    "#### Other aggregations\n",
    "- skewness and kurtosis\n",
    "- Covariance and Correlation\n",
    "\n",
    "#### Window Functions\n",
    "- You can also use window functions to carry out some unique aggregations by either computing some aggregation on a specific “window” of data, which you define by using a reference to the current data. \n",
    "- This window specification determines which rows will be passed in to this function.\n",
    "- A group-by takes data, and every row can go only into one grouping. \n",
    "- A window function calculates a return value for every input row of a table based on a group of rows, called a frame.\n",
    "- Each row can fall into one or more frames. \n",
    "- A common use case is to take a look at a rolling average of some value for which each row represents one day. \n",
    "- If you were to do this, each row would end up in seven different frames.\n",
    "-  Spark supports three kinds of window functions: \n",
    "1. ranking functions, \n",
    "2. analytic functions,\n",
    "3. aggregate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204258ad",
   "metadata": {},
   "source": [
    "## Joins\n",
    "- A join brings together two sets of data, the left and the right, by comparing the value of one or more keys of the left and right and evaluating the result of a join expression that determines whether Spark should bring together the left set of data with the right set of data. \n",
    "- The most common join expression, an equi-join, compares whether the specified keys in your left and right datasets are equal. If they are equal, Spark will combine the left and right datasets. The opposite is true for keys that do not match; Spark discards the rows that do not have matching keys. \n",
    "- Spark also allows for much more sophsticated join policies in addition to equi-joins. \n",
    "\n",
    "#### Join Types\n",
    "- Whereas the join expression determines whether two rows should join, the join type determines what should be in the result set. \n",
    "- There are a variety of different join types available in Spark for you to use:\n",
    "1. Inner joins (keep rows with keys that exist in the left and right datasets)\n",
    "2. Outer joins (keep rows with keys in either the left or right datasets)\n",
    "3. Left outer joins (keep rows with keys in the left dataset)\n",
    "4. Right outer joins (keep rows with keys in the right dataset)\n",
    "5. Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n",
    "6. Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n",
    "7. Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n",
    "8. Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2d82dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data \n",
    "person = spark.createDataFrame([\n",
    "(0, \"Bill Chambers\", 0, [100]),\n",
    "(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "(2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")])\\\n",
    ".toDF(\"id\", \"status\")\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba295c",
   "metadata": {},
   "source": [
    "#### Inner Join\n",
    "- Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together) only the rows that evaluate to true. \n",
    "- In the following example, we join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab7eb3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inner join\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749179e",
   "metadata": {},
   "source": [
    "#### Outer Join\n",
    "- Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. - If there is no equivalent row in either the left or right DataFrame, Spark will insert null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b1809af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|NULL|            NULL|            NULL|           NULL|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e1fad",
   "metadata": {},
   "source": [
    "#### Left Outer Joins\n",
    "- Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. \n",
    "- If there is no equivalent row in the right DataFrame, Spark will insert null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdedd774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|NULL|            NULL|            NULL|           NULL|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca41ec",
   "metadata": {},
   "source": [
    "#### Cross (Cartesian) Joins\n",
    "- The last of our joins are cross-joins or cartesian products. \n",
    "- Cross-joins in simplest terms are inner joins that do not specify a predicate. \n",
    "- Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. \n",
    "- If you have 1,000 rows in each DataFrame, the crossjoin of these will result in 1,000,000 (1,000 x 1,000) rows. \n",
    "- For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00baf816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3aacf",
   "metadata": {},
   "source": [
    "## Datasources\n",
    "Include:\n",
    "1. csv files\n",
    "2. json files\n",
    "3. orc files\n",
    "4. parquet files\n",
    "5. sql databases\n",
    "6. text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and write csv files\n",
    "file_name = os.path.join(BASE_DIR, \"flight-data\", \"csv\", \"2010-summary.csv\")\n",
    "\n",
    "csvFile = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(file_name)\n",
    "\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n",
    ".save(os.path.join(BASE_DIR, \"my-tsv-file.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(BASE_DIR, \"flight-data\", \"json\", \"2010-summary.json\")\n",
    "\n",
    "jsonFile = spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(file_name)\n",
    "\n",
    "jsonFile.write.format(\"json\").mode(\"overwrite\").save(os.path.join(BASE_DIR, \"my-json-file.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(BASE_DIR, \"flight-data\", \"parquet\", \"2010-summary.parquet\")\n",
    "\n",
    "parkFile = spark.read.format(\"parquet\")\\\n",
    ".load(file_name)\n",
    "\n",
    "parkFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    ".save(os.path.join(BASE_DIR, \"my-parquet-file.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aed1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(BASE_DIR, \"flight-data\", \"orc\", \"2010-summary.orc\")\n",
    "\n",
    "orcFile = spark.read.format(\"orc\").load(file_name)\n",
    "orcFile.write.format(\"orc\").mode(\"overwrite\").save(os.path.join(BASE_DIR, \"my-orc-file.orc\"), compression=\"SNAPPY\") # compression=\"SNAPPY\"data/my-json-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23aa15",
   "metadata": {},
   "source": [
    "## Spark and Machine learning\n",
    "- Machine learning tasks include:\n",
    "1. Supervised learning, including classification and regression, where the goal is to predict a label for each data point based on various features.\n",
    "2. Recommendation engines to suggest products to users based on behavior.\n",
    "3. Unsupervised learning, including clustering, anomaly detection, and topic modeling, where the goal is to discover structure in the data.\n",
    "4. Graph analytics tasks such as searching for patterns in a social network.\n",
    "\n",
    "#### Supervised Learning\n",
    "- The goal is simple: using historical data that already has labels (often called the dependent variables), train a modelto predict the values of those labels based on various features of the data points. \n",
    "- One example would be to predict a person’s income (the dependent variable) based on age (a feature).\n",
    "- Divided into:\n",
    "1. Classification:\n",
    "Classification is the act of training an algorithm to predict a dependent variable that is categorical (belonging to a discrete, finite set of\n",
    "values). The most common case is binary classification, where our resulting model will make a prediction that a given item belongs to one of two groups. The canonical example is classifying email spam. \n",
    "2. Regression\n",
    "In regression, we instead try to predict a continuous variable (a real number). In simplest terms, rather than predicting a category, we want to predict a value on a number line. \n",
    "\n",
    "#### Unsupervised Learning\n",
    "- Unsupervised learning is the act of trying to find patterns or discover the underlying structure in a given set of data. This differs from supervised learning because there is no dependent variable (label) to predict.\n",
    "\n",
    "## The Machine Learning workflow\n",
    "- The overall process involves, the following steps (with some variation):\n",
    "1. Gathering and collecting the relevant data for your task.\n",
    "2. Cleaning and inspecting the data to better understand it.\n",
    "3. Performing feature engineering to allow the algorithm to leverage the data in a suitable form (e.g., converting the data to numerical vectors).\n",
    "4. Using a portion of this data as a training set to train one or more algorithms to generate some candidate models.\n",
    "5. Evaluating and comparing models against your success criteria by objectively measuring results on a subset of the same data that was not used for training. This allows you to better understand how your model may perform in the wild.\n",
    "6. Leveraging the insights from the above process and/or using the model to make predictions, detect anomalies, or solve more general business challenges.\n",
    "\n",
    "#### What Is MLlib?\n",
    "- MLlib is a package, built on and included in Spark, that provides interfaces for gathering and cleaning data, feature engineering and feature selection, training and tuning large-scale supervised and unsupervised machine learning models, and using those models in production.\n",
    "#### High-Level MLlib Concepts\n",
    "1. Transformers\n",
    "- Transformers are functions that convert raw data in some way. \n",
    "- This might be to create a new interaction variable (from two other variables), normalize a column, or simply change an Integer into a Double type to be input into a model. \n",
    "- An example of a transformer is one that converts string categorical variables into numerical values that can be used in MLlib.\n",
    "- Transformers are primarily used in preprocessing and feature engineering. \n",
    "- Transformers take a DataFrame as input and produce a new DataFrame as output\n",
    "2. Estimators\n",
    "- Estimators are one of two kinds of things. \n",
    "- First, estimators can be a kind of transformer that is initialized with data. \n",
    "- For instance, to normalize numerical data we’ll need to initialize our transformation with some information about the current values in the column we would like to normalize. \n",
    "- This requires two passes over our data—the initial pass generates the initialization values and the second actually applies the generated function over the data. \n",
    "- In the Spark’s nomenclature, algorithms that allow users to train a model from data are also referred to as estimators.\n",
    "3. Evaluators\n",
    "- An evaluator allows us to see how a given model performs according to criteria we specify like a receiver operating characteristic (ROC) curve. \n",
    "- After we use an evaluator to select the best model from the ones we tested, we can then use that model to make predictions.\n",
    "4. Pipelines\n",
    "- From a high level we can specify each of the transformations, estimations, and evaluations one by one, but it is often easier to specify our steps as stages in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8436d",
   "metadata": {},
   "source": [
    "#### ML Feature Preprocessing\n",
    "- In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent the features.\n",
    "- In the case of recommendation, you want to get your data into a column of users, a column of items (say movies or books), and a column of ratings.\n",
    "- In the case of unsupervised learning, a column of type Vector (either dense or sparse) is needed to represent the features.\n",
    "- In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame\n",
    "of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e162c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/15 15:16:10 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/retail-data/by-day/*.csv.\n",
      "java.io.FileNotFoundException: File data/retail-data/by-day/*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "sales = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"data/retail-data/by-day/*.csv\")\\\n",
    ".coalesce(5)\\\n",
    ".where(\"Description IS NOT NULL\")\n",
    "fakeIntDF = spark.read.parquet(os.path.join(BASE_DIR, \"simple-ml-integers\"))\n",
    "simpleDF = spark.read.json(os.path.join(BASE_DIR, \"simple-ml\"))\n",
    "scaleDF = spark.read.parquet(os.path.join(BASE_DIR, \"simple-ml-scaling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c396a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4bd31",
   "metadata": {},
   "source": [
    "#### VectorAssembler\n",
    "- It helps concatenate all your features into one big vector you can then pass into an estimator. \n",
    "- It’s used typically in the last step of a machine learning pipeline and takes as input a number of columns of Boolean, Double, or Vector.\n",
    "- This is particularly helpful if you’re going to perform a number of manipulations using a variety of transformers and need to gather all of those results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1718afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_f133ab307284__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32013c",
   "metadata": {},
   "source": [
    "##### Bucketing\n",
    "- This will split a given continuous feature into the buckets of your designation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9c2ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_0818ebf16cb2__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c2948",
   "metadata": {},
   "source": [
    "##### StandardScaler\n",
    "- The StandardScaler standardizes a set of features to have zero mean and a standard deviationof 1. \n",
    "- The flag withStd will scale the data to unit standard deviation while the flag withMean (false by default) will center the data prior to scaling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d05ee839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_c65d2672f8fb__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38120a84",
   "metadata": {},
   "source": [
    "#### MinMaxScaler\n",
    "- The MinMaxScaler will scale the values in a vector (component wise) to the proportional values on a scale from a given min value to a max value. \n",
    "- If you specify the minimum value to be 0 and the maximum value to be 1, then all the values will fall in between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9dbd42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_b2220de730ad__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b339a2",
   "metadata": {},
   "source": [
    "#### ElementwiseProduct\n",
    "- The ElementwiseProduct allows us to scale each value in a vector by an arbitrary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfd4ebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_dd80c417713a__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    ".setScalingVec(scaleUpVec)\\\n",
    ".setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bfcc9",
   "metadata": {},
   "source": [
    "#### StringIndexer\n",
    "- The simplest way to index is via the StringIndexer, which maps strings to different numerical IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79f17912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b515ad8",
   "metadata": {},
   "source": [
    "## Regression\n",
    "- Regression is the act of predicting a real number (or continuous variable) from a set of features (represented as numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(os.path.join(BASE_DIR, \"regression\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1447c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print (lr.explainParams())\n",
    "lrModel = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31b75253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.1280504658561019|\n",
      "|-0.14468269261572098|\n",
      "| -0.4190383262242056|\n",
      "| -0.4190383262242056|\n",
      "|  0.8547088792080308|\n",
      "+--------------------+\n",
      "\n",
      "5\n",
      "[0.5000000000000001, 0.4315295810362787, 0.3132335933881021, 0.312256926665541, 0.3091506081983029, 0.3091505893348027]\n",
      "0.47308424392175985\n",
      "0.720239122691221\n"
     ]
    }
   ],
   "source": [
    "summary = lrModel.summary\n",
    "summary.residuals.show()\n",
    "print (summary.totalIterations)\n",
    "print (summary.objectiveHistory)\n",
    "print (summary.rootMeanSquaredError)\n",
    "print (summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321e184",
   "metadata": {},
   "source": [
    "#### K-Means Clustering\n",
    "- In this algorithm, a user-specified number of clusters (ὅ) are randomly assigned to different points in the dataset. \n",
    "- The unassigned points are then “assigned” to a cluster based on their proximity (measured in Euclidean distance) to the previously assigned point. \n",
    "- Once this assignment happens, the center of this cluster (called the centroid) is computed, and the process repeats. All points are assigned to a particular centroid, and a new centroid is computed. \n",
    "- We repeat this process for a finite number of iterations or until convergence (i.e., when our centroid locations stop changing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3716553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/15 15:52:06 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/retail-data/by-day/*.csv.\n",
      "java.io.FileNotFoundException: File data/retail-data/by-day/*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor236.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, features: vector]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler()\\\n",
    ".setInputCols([\"Quantity\", \"UnitPrice\"])\\\n",
    ".setOutputCol(\"features\")\n",
    "sales = va.transform(spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(os.path.join(BASE_DIR, \"retail-data\", \"by-day\", \"*.csv\"))\n",
    ".limit(50)\n",
    ".coalesce(1)\n",
    ".where(\"Description IS NOT NULL\"))\n",
    "sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3713b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 5)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 1997370414816703631)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "km = KMeans().setK(5)\n",
    "print (km.explainParams())\n",
    "kmModel = km.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e59cd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 2, 8, 11, 19]\n",
      "Cluster Centers: \n",
      "[12.    0.93]\n",
      "[48.    1.32]\n",
      "[ 2.5     11.24375]\n",
      "[24.36363636  0.94636364]\n",
      "[5.21052632 3.74105263]\n"
     ]
    }
   ],
   "source": [
    "summary = kmModel.summary\n",
    "print (summary.clusterSizes) # number of points\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print (center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8993b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
